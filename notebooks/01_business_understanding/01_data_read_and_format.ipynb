{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef921af",
   "metadata": {},
   "source": [
    "# Data Read and Formatting\n",
    "\n",
    "Notebook para la lectura de los datos y para su modificación de formato en archivos csv, con tablas individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34652fee",
   "metadata": {},
   "source": [
    "## Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85140d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio actual: /home/ferrus/Documents/university/semester_x/MA2003B/PROYECTO_MA2003B/notebooks/01_business_understanding\n",
      "Raíz del proyecto: /home/ferrus/Documents/university/semester_x/MA2003B/PROYECTO_MA2003B\n",
      "CWD cambiado a raíz del proyecto\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "print(\"Directorio actual:\", os.getcwd())\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "while not (project_root / '.git').exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(f\"Raíz del proyecto: {project_root}\")\n",
    "print(\"CWD cambiado a raíz del proyecto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6be1fc",
   "metadata": {},
   "source": [
    "# Esquema de consolidación para archivos .xlsx 2022 - 2024\n",
    "\n",
    "Se utiliza la versión procesada que separa las distintas tablas indexadas en distintas hojas de cálculo. Es primordial separar las tablas de los datos de 2023 para lograr una unidad en la metodología de importación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3904bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado como: data/interim/DATOS_HISTORICOS_2023_2024_CONCATENADO.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo original\n",
    "archivo = r\"data/interim/DATOS HISTÓRICOS 2023_2024_SEPARADO.xlsx\"\n",
    "\n",
    "# Cargar todas las hojas en un diccionario {nombre_hoja: DataFrame}\n",
    "hojas = pd.read_excel(archivo, sheet_name=None)\n",
    "\n",
    "# Lista para guardar los DataFrames concatenados\n",
    "dfs = []\n",
    "\n",
    "for nombre_hoja, df in hojas.items():\n",
    "    # Poner todos los nombres de columnas en minúsculas\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Cambiar \"date\" a \"date_index\" si existe\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.rename(columns={\"date\": \"date_index\"})\n",
    "    \n",
    "    # Agregar la columna 'estacion'\n",
    "    df[\"estacion\"] = nombre_hoja\n",
    "    \n",
    "    # Limpiar caracteres no imprimibles de strings\n",
    "    \n",
    "    # Excluir \"CATÁLOGO\" de la unión\n",
    "    if nombre_hoja != \"CATÁLOGO\":\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenar todas las hojas excepto \"CATÁLOGO\"\n",
    "df_final = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Guardar en un nuevo archivo con una sola hoja (usando xlsxwriter)\n",
    "nuevo_archivo = \"data/interim/DATOS_HISTORICOS_2023_2024_CONCATENADO.xlsx\"\n",
    "df_final.to_excel(nuevo_archivo, sheet_name=\"datos\", index=False, engine=\"xlsxwriter\")\n",
    "\n",
    "print(\"Archivo guardado como:\", nuevo_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370241fe",
   "metadata": {},
   "source": [
    "# Esquema de lectura y tipado del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2c3c0",
   "metadata": {},
   "source": [
    "## Lectura de archivos post primer organización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cfd73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_22 = r\"data/interim/DATOS_HISTORICOS_2022_2023_CONCATENADO.xlsx\"\n",
    "path_23 = r\"data/interim/DATOS_HISTORICOS_2023_2024_CONCATENADO.xlsx\"\n",
    "path_24 = r\"data/interim/DATOS_HISTORICOS_2024_CONCATENADO.csv\"\n",
    "\n",
    "df_22 = pd.read_excel(path_22)\n",
    "df_23 = pd.read_excel(path_23)\n",
    "df_24 = pd.read_csv(path_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f40dd5",
   "metadata": {},
   "source": [
    "### Verificación de campos y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09dfe593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23.columns == df_22.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "865dd32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_index', 'co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm2.5', 'prs',\n",
       "       'rainf', 'rh', 'so2', 'sr', 'tout', 'wsr', 'wdr', 'estacion',\n",
       "       'archivo_origen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93bb905c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'co (ppm)', 'no (ppb)', 'no2 (ppb)', 'nox (ppb)', 'o3 (ppb)',\n",
       "       'pm10 (ug/m3)', 'pm2.5 (ug/m3)', 'prs (mmhg)', 'rainf (mm/h)', 'rh (%)',\n",
       "       'so2 (ppb)', 'sr (kw/m2)', 'tout (ºc)', 'wsr (km/h)', 'wdr (azimutal)',\n",
       "       'estacion', 'archivo_origen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86fbf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_renombre_24 = {'date':'date_index',\n",
    "                   'co (ppm)':'co',\n",
    "                   'no (ppb)':'no',\n",
    "                   'no2 (ppb)':'no2',\n",
    "                   'nox (ppb)': 'nox',\n",
    "                   'o3 (ppb)' : 'o3',\n",
    "                   'pm10 (ug/m3)': 'pm10',\n",
    "                   'pm2.5 (ug/m3)':'pm2.5',\n",
    "                   'prs (mmhg)':'prs',\n",
    "                   'rainf (mm/h)':'rainf',\n",
    "                   'rh (%)':'rh',\n",
    "                   'so2 (ppb)':'so2',\n",
    "                   'sr (kw/m2)':'sr',\n",
    "                   'tout (ºc)':'tout',\n",
    "                   'wsr (km/h)':'wsr',\n",
    "                   'wdr (azimutal)':'wdr',\n",
    "                   'estacion':'estacion'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c1cc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24_renamed = df_24.rename(columns=mapa_renombre_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7649dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23.columns == df_24_renamed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4fbd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22['archivo_origen'] = \"DATOS_HISTORICOS_2022_2023_CONCATENADO\"\n",
    "df_23['archivo_origen'] = 'DATOS_HISTORICOS_2023_2024_CONCATENADO.xlsx'\n",
    "df_24['archivo_origen'] = 'DATOS_HISTORICOS_2024_CONCATENADO.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba1119",
   "metadata": {},
   "source": [
    "### Esquema de tipado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bae425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import warnings\n",
    "\n",
    "def optimizar_tipos_datos(df: pd.DataFrame, \n",
    "                         columnas_fecha: Optional[List[str]] = None,\n",
    "                         columnas_string: Optional[List[str]] = None,\n",
    "                         formato_fecha: Optional[str] = None,\n",
    "                         reducir_memoria: bool = True,\n",
    "                         verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimiza los tipos de datos de un DataFrame de forma inteligente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame a optimizar\n",
    "    columnas_fecha : Optional[List[str]]\n",
    "        Lista específica de columnas a tratar como fecha. Si None, detecta automáticamente.\n",
    "    columnas_string : Optional[List[str]]\n",
    "        Lista específica de columnas a tratar como string. Si None, detecta automáticamente.\n",
    "    formato_fecha : Optional[str]\n",
    "        Formato específico para parsing de fechas (ej: '%Y-%m-%d %H:%M:%S')\n",
    "    reducir_memoria : bool\n",
    "        Si True, optimiza tipos numéricos para reducir memoria\n",
    "    verbose : bool\n",
    "        Si True, muestra información detallada del proceso\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame con tipos optimizados\n",
    "    \"\"\"\n",
    "    \n",
    "    df_optimized = df.copy()\n",
    "    cambios_realizados = {}\n",
    "    memoria_original = df.memory_usage(deep=True).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Memoria original: {memoria_original / 1024**2:.2f} MB\")\n",
    "        print(f\"Columnas a procesar: {list(df.columns)}\")\n",
    "    \n",
    "    # 1. DETECTAR Y CONVERTIR COLUMNAS DE FECHA\n",
    "    columnas_fecha_detectadas = _detectar_columnas_fecha(df_optimized, columnas_fecha)\n",
    "    \n",
    "    for col in columnas_fecha_detectadas:\n",
    "        tipo_original = str(df_optimized[col].dtype)\n",
    "        try:\n",
    "            df_optimized[col] = _convertir_a_datetime(df_optimized[col], formato_fecha)\n",
    "            cambios_realizados[col] = f\"{tipo_original} → datetime64[ns]\"\n",
    "            if verbose:\n",
    "                print(f\"✓ {col}: convertida a datetime\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"✗ {col}: falló conversión a datetime - {str(e)}\")\n",
    "    \n",
    "    # 2. DETECTAR Y CONVERTIR COLUMNAS STRING\n",
    "    columnas_string_detectadas = _detectar_columnas_string(df_optimized, columnas_string, columnas_fecha_detectadas)\n",
    "    \n",
    "    for col in columnas_string_detectadas:\n",
    "        tipo_original = str(df_optimized[col].dtype)\n",
    "        if df_optimized[col].dtype != 'object':\n",
    "            df_optimized[col] = df_optimized[col].astype('string')\n",
    "            cambios_realizados[col] = f\"{tipo_original} → string\"\n",
    "            if verbose:\n",
    "                print(f\"✓ {col}: convertida a string\")\n",
    "    \n",
    "    # 3. OPTIMIZAR COLUMNAS NUMÉRICAS\n",
    "    columnas_numericas = _identificar_columnas_numericas(df_optimized, columnas_fecha_detectadas, columnas_string_detectadas)\n",
    "    \n",
    "    for col in columnas_numericas:\n",
    "        tipo_original = str(df_optimized[col].dtype)\n",
    "        \n",
    "        try:\n",
    "            # Primero convertir a numérico si es necesario\n",
    "            if not pd.api.types.is_numeric_dtype(df_optimized[col]):\n",
    "                df_optimized[col] = pd.to_numeric(df_optimized[col], errors='coerce')\n",
    "            \n",
    "            tipo_optimizado = _optimizar_columna_numerica(df_optimized[col], reducir_memoria)\n",
    "            \n",
    "            if tipo_optimizado != tipo_original:\n",
    "                # Intentar conversión con manejo de errores\n",
    "                try:\n",
    "                    df_optimized[col] = df_optimized[col].astype(tipo_optimizado)\n",
    "                    cambios_realizados[col] = f\"{tipo_original} → {tipo_optimizado}\"\n",
    "                    if verbose:\n",
    "                        print(f\"✓ {col}: optimizada a {tipo_optimizado}\")\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    # Si falla, usar float64 como fallback\n",
    "                    df_optimized[col] = df_optimized[col].astype('float64')\n",
    "                    cambios_realizados[col] = f\"{tipo_original} → float64 (fallback)\"\n",
    "                    if verbose:\n",
    "                        print(f\"! {col}: fallback a float64 - {str(e)[:50]}...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"✗ {col}: error en optimización - {str(e)[:50]}...\")\n",
    "    \n",
    "    # 4. REPORTE FINAL\n",
    "    memoria_final = df_optimized.memory_usage(deep=True).sum()\n",
    "    reduccion_memoria = ((memoria_original - memoria_final) / memoria_original) * 100\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nMemoria final: {memoria_final / 1024**2:.2f} MB\")\n",
    "        print(f\"Reducción de memoria: {reduccion_memoria:.1f}%\")\n",
    "        print(f\"Cambios realizados: {len(cambios_realizados)} columnas\")\n",
    "    \n",
    "    return df_optimized\n",
    "\n",
    "def _detectar_columnas_fecha(df: pd.DataFrame, columnas_especificas: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Detecta columnas que contienen fechas.\"\"\"\n",
    "    \n",
    "    if columnas_especificas:\n",
    "        return [col for col in columnas_especificas if col in df.columns]\n",
    "    \n",
    "    columnas_fecha = []\n",
    "    \n",
    "    # Palabras clave que sugieren fechas\n",
    "    keywords_fecha = ['date', 'time', 'fecha', 'hora', 'timestamp', 'created', 'updated']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Por nombre de columna\n",
    "        if any(keyword in col.lower() for keyword in keywords_fecha):\n",
    "            columnas_fecha.append(col)\n",
    "            continue\n",
    "        \n",
    "        # Por contenido (muestreo)\n",
    "        if df[col].dtype == 'object':\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if len(muestra) > 0:\n",
    "                # Intentar detectar patrones de fecha\n",
    "                fecha_count = 0\n",
    "                for valor in muestra:\n",
    "                    if _es_posible_fecha(str(valor)):\n",
    "                        fecha_count += 1\n",
    "                \n",
    "                if fecha_count / len(muestra) > 0.7:  # 70% parecen fechas\n",
    "                    columnas_fecha.append(col)\n",
    "    \n",
    "    return columnas_fecha\n",
    "\n",
    "def _es_posible_fecha(valor: str) -> bool:\n",
    "    \"\"\"Verifica si un string podría ser una fecha.\"\"\"\n",
    "    \n",
    "    # Patrones comunes de fecha\n",
    "    patrones_fecha = [\n",
    "        r'\\d{4}-\\d{1,2}-\\d{1,2}',      # YYYY-MM-DD\n",
    "        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',    # MM/DD/YYYY o DD/MM/YYYY\n",
    "        r'\\d{4}/\\d{1,2}/\\d{1,2}',      # YYYY/MM/DD\n",
    "        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',    # MM-DD-YYYY\n",
    "    ]\n",
    "    \n",
    "    import re\n",
    "    return any(re.search(patron, valor) for patron in patrones_fecha)\n",
    "\n",
    "def _convertir_a_datetime(serie: pd.Series, formato: Optional[str] = None) -> pd.Series:\n",
    "    \"\"\"Convierte una serie a datetime de forma robusta.\"\"\"\n",
    "    \n",
    "    if formato:\n",
    "        return pd.to_datetime(serie, format=formato, errors='coerce')\n",
    "    \n",
    "    # Intentar conversión automática\n",
    "    try:\n",
    "        return pd.to_datetime(serie, errors='coerce', infer_datetime_format=True)\n",
    "    except:\n",
    "        # Fallback: intentar formatos comunes\n",
    "        formatos_comunes = [\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%m/%d/%Y %H:%M:%S',\n",
    "            '%m/%d/%Y',\n",
    "            '%d/%m/%Y',\n",
    "            '%Y/%m/%d'\n",
    "        ]\n",
    "        \n",
    "        for fmt in formatos_comunes:\n",
    "            try:\n",
    "                return pd.to_datetime(serie, format=fmt, errors='coerce')\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Si todo falla, conversión con errores='coerce'\n",
    "        return pd.to_datetime(serie, errors='coerce')\n",
    "\n",
    "def _detectar_columnas_string(df: pd.DataFrame, \n",
    "                             columnas_especificas: Optional[List[str]] = None,\n",
    "                             excluir_columnas: List[str] = None) -> List[str]:\n",
    "    \"\"\"Detecta columnas que deben ser tratadas como string.\"\"\"\n",
    "    \n",
    "    if excluir_columnas is None:\n",
    "        excluir_columnas = []\n",
    "    \n",
    "    if columnas_especificas:\n",
    "        return [col for col in columnas_especificas if col in df.columns and col not in excluir_columnas]\n",
    "    \n",
    "    columnas_string = []\n",
    "    \n",
    "    # Palabras clave que sugieren strings\n",
    "    keywords_string = ['name', 'nombre', 'id', 'codigo', 'description', 'descripcion', \n",
    "                      'category', 'categoria', 'type', 'tipo', 'status', 'estado',\n",
    "                      'estacion', 'location', 'ubicacion']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in excluir_columnas:\n",
    "            continue\n",
    "            \n",
    "        # Por nombre de columna\n",
    "        if any(keyword in col.lower() for keyword in keywords_string):\n",
    "            columnas_string.append(col)\n",
    "            continue\n",
    "        \n",
    "        # Por tipo actual\n",
    "        if df[col].dtype == 'object':\n",
    "            # Verificar si no es numérica\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if len(muestra) > 0:\n",
    "                numerico_count = 0\n",
    "                for valor in muestra:\n",
    "                    try:\n",
    "                        float(str(valor))\n",
    "                        numerico_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if numerico_count / len(muestra) < 0.8:  # Menos del 80% son numéricos\n",
    "                    columnas_string.append(col)\n",
    "    \n",
    "    return columnas_string\n",
    "\n",
    "def _identificar_columnas_numericas(df: pd.DataFrame, \n",
    "                                   excluir_fecha: List[str],\n",
    "                                   excluir_string: List[str]) -> List[str]:\n",
    "    \"\"\"Identifica columnas que deben ser tratadas como numéricas.\"\"\"\n",
    "    \n",
    "    excluir_total = set(excluir_fecha + excluir_string)\n",
    "    \n",
    "    columnas_numericas = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in excluir_total:\n",
    "            continue\n",
    "        \n",
    "        # Si ya es numérica o si es object que se puede convertir\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) or df[col].dtype == 'object':\n",
    "            # Verificar si se puede convertir a numérico\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if len(muestra) > 0:\n",
    "                convertible_count = 0\n",
    "                for valor in muestra:\n",
    "                    try:\n",
    "                        pd.to_numeric(valor)\n",
    "                        convertible_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if convertible_count / len(muestra) > 0.8:  # 80% convertibles\n",
    "                    columnas_numericas.append(col)\n",
    "    \n",
    "    return columnas_numericas\n",
    "\n",
    "def _optimizar_columna_numerica(serie: pd.Series, reducir_memoria: bool = True) -> str:\n",
    "    \"\"\"Determina el tipo numérico óptimo para una serie.\"\"\"\n",
    "    \n",
    "    # Convertir a numérico si no lo es\n",
    "    if not pd.api.types.is_numeric_dtype(serie):\n",
    "        serie_num = pd.to_numeric(serie, errors='coerce')\n",
    "    else:\n",
    "        serie_num = serie\n",
    "    \n",
    "    if not reducir_memoria:\n",
    "        return 'float64'\n",
    "    \n",
    "    # Verificar si hay valores infinitos o solo NaN\n",
    "    datos_validos = serie_num.dropna()\n",
    "    if len(datos_validos) == 0:\n",
    "        return 'float64'  # Solo NaN, usar float64\n",
    "    \n",
    "    # Verificar si hay valores infinitos\n",
    "    if np.isinf(datos_validos).any():\n",
    "        return 'float64'  # Hay infinitos, usar float64\n",
    "    \n",
    "    # Verificar si son todos enteros (sin decimales)\n",
    "    try:\n",
    "        es_entero = datos_validos.apply(lambda x: float(x).is_integer()).all()\n",
    "    except:\n",
    "        es_entero = False\n",
    "    \n",
    "    if es_entero and len(datos_validos) > 0:\n",
    "        # Es entero - verificar rangos\n",
    "        min_val = datos_validos.min()\n",
    "        max_val = datos_validos.max()\n",
    "        \n",
    "        # Si hay NaN en la serie original, usar tipos nullable\n",
    "        tiene_nulos = serie_num.isna().any()\n",
    "        \n",
    "        if tiene_nulos:\n",
    "            # Usar tipos nullable de pandas (Int64, etc.)\n",
    "            if min_val >= 0:  # Unsigned\n",
    "                if max_val <= 255:\n",
    "                    return 'UInt8'\n",
    "                elif max_val <= 65535:\n",
    "                    return 'UInt16'\n",
    "                elif max_val <= 4294967295:\n",
    "                    return 'UInt32'\n",
    "                else:\n",
    "                    return 'UInt64'\n",
    "            else:  # Signed\n",
    "                if min_val >= -128 and max_val <= 127:\n",
    "                    return 'Int8'\n",
    "                elif min_val >= -32768 and max_val <= 32767:\n",
    "                    return 'Int16'\n",
    "                elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                    return 'Int32'\n",
    "                else:\n",
    "                    return 'Int64'\n",
    "        else:\n",
    "            # Sin nulos, usar tipos estándar\n",
    "            if min_val >= 0:  # Unsigned\n",
    "                if max_val <= 255:\n",
    "                    return 'uint8'\n",
    "                elif max_val <= 65535:\n",
    "                    return 'uint16'\n",
    "                elif max_val <= 4294967295:\n",
    "                    return 'uint32'\n",
    "                else:\n",
    "                    return 'uint64'\n",
    "            else:  # Signed\n",
    "                if min_val >= -128 and max_val <= 127:\n",
    "                    return 'int8'\n",
    "                elif min_val >= -32768 and max_val <= 32767:\n",
    "                    return 'int16'\n",
    "                elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                    return 'int32'\n",
    "                else:\n",
    "                    return 'int64'\n",
    "    else:\n",
    "        # Es float o tiene decimales\n",
    "        return 'float32' if reducir_memoria else 'float64'\n",
    "\n",
    "def aplicar_a_diccionario_dataframes(dataframes_dict: Dict[str, pd.DataFrame], \n",
    "                                   **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplica optimización de tipos a un diccionario de DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframes_dict : Dict[str, pd.DataFrame]\n",
    "        Diccionario con DataFrames a optimizar\n",
    "    **kwargs : argumentos para optimizar_tipos_datos\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, pd.DataFrame] : Diccionario con DataFrames optimizados\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframes_optimizados = {}\n",
    "    \n",
    "    for nombre, df in dataframes_dict.items():\n",
    "        print(f\"\\n--- Optimizando {nombre} ---\")\n",
    "        df_optimizado = optimizar_tipos_datos(df, **kwargs)\n",
    "        dataframes_optimizados[nombre] = df_optimizado\n",
    "    \n",
    "    return dataframes_optimizados\n",
    "\n",
    "# EJEMPLO DE USO ESPECÍFICO PARA TU CASO\n",
    "def optimizar_estaciones_itesm(dataframes_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Optimización específica para los DataFrames de estaciones ITESM.\n",
    "    \"\"\"\n",
    "    \n",
    "    return aplicar_a_diccionario_dataframes(\n",
    "        dataframes_dict,\n",
    "        columnas_fecha=['date_index'],\n",
    "        columnas_string=['estacion'],\n",
    "        reducir_memoria=True,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2885d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria original: 53.23 MB\n",
      "Columnas a procesar: ['date_index', 'co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm2.5', 'prs', 'rainf', 'rh', 'so2', 'sr', 'tout', 'wsr', 'wdr', 'estacion', 'archivo_origen']\n",
      "✓ date_index: convertida a datetime\n",
      "✓ co: optimizada a float32\n",
      "✓ no: optimizada a float32\n",
      "✓ no2: optimizada a float32\n",
      "✓ nox: optimizada a float32\n",
      "✓ o3: optimizada a float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5478/1269784384.py:163: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(serie, errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pm10: optimizada a float32\n",
      "✓ pm2.5: optimizada a float32\n",
      "✓ prs: optimizada a float32\n",
      "✓ rainf: optimizada a float32\n",
      "✓ rh: optimizada a float32\n",
      "✓ so2: optimizada a float32\n",
      "✓ sr: optimizada a float32\n",
      "✓ tout: optimizada a float32\n",
      "✓ wsr: optimizada a float32\n",
      "✓ wdr: optimizada a Int16\n",
      "\n",
      "Memoria final: 41.26 MB\n",
      "Reducción de memoria: 22.5%\n",
      "Cambios realizados: 16 columnas\n",
      "Memoria original: 54.88 MB\n",
      "Columnas a procesar: ['date_index', 'co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm2.5', 'prs', 'rainf', 'rh', 'so2', 'sr', 'tout', 'wsr', 'wdr', 'estacion', 'archivo_origen']\n",
      "✓ date_index: convertida a datetime\n",
      "✓ co: optimizada a float32\n",
      "✓ no: optimizada a float32\n",
      "✓ no2: optimizada a float32\n",
      "✓ nox: optimizada a float32\n",
      "✓ o3: optimizada a float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5478/1269784384.py:163: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(serie, errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pm10: optimizada a float32\n",
      "✓ pm2.5: optimizada a float32\n",
      "✓ prs: optimizada a float32\n",
      "✓ rainf: optimizada a float32\n",
      "✓ rh: optimizada a Int16\n",
      "✓ so2: optimizada a float32\n",
      "✓ sr: optimizada a float32\n",
      "✓ tout: optimizada a float32\n",
      "✓ wsr: optimizada a float32\n",
      "✓ wdr: optimizada a UInt16\n",
      "\n",
      "Memoria final: 42.58 MB\n",
      "Reducción de memoria: 22.4%\n",
      "Cambios realizados: 16 columnas\n",
      "Memoria original: 41.56 MB\n",
      "Columnas a procesar: ['date_index', 'co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm2.5', 'prs', 'rainf', 'rh', 'so2', 'sr', 'tout', 'wsr', 'wdr', 'estacion', 'archivo_origen']\n",
      "✓ date_index: convertida a datetime\n",
      "✓ co: optimizada a float32\n",
      "✓ no: optimizada a float32\n",
      "✓ no2: optimizada a float32\n",
      "✓ nox: optimizada a float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5478/1269784384.py:163: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(serie, errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ o3: optimizada a float32\n",
      "✓ pm10: optimizada a float32\n",
      "✓ pm2.5: optimizada a float32\n",
      "✓ prs: optimizada a float32\n",
      "✓ rainf: optimizada a float32\n",
      "✓ rh: optimizada a float32\n",
      "✓ so2: optimizada a float32\n",
      "✓ sr: optimizada a float32\n",
      "✓ tout: optimizada a float32\n",
      "✓ wsr: optimizada a float32\n",
      "✓ wdr: optimizada a float32\n",
      "\n",
      "Memoria final: 26.48 MB\n",
      "Reducción de memoria: 36.3%\n",
      "Cambios realizados: 16 columnas\n"
     ]
    }
   ],
   "source": [
    "df_22_opt = optimizar_tipos_datos(df = df_22, \n",
    "                         columnas_fecha = ['date_index'],\n",
    "                         columnas_string = ['estacion','archivo_origen'],\n",
    "                         reducir_memoria = True,\n",
    "                         verbose = True)\n",
    "df_23_opt = optimizar_tipos_datos(df = df_23, \n",
    "                         columnas_fecha = ['date_index'],\n",
    "                         columnas_string = ['estacion','archivo_origen'],\n",
    "                         reducir_memoria = True,\n",
    "                         verbose = True)\n",
    "df_24_opt = optimizar_tipos_datos(df = df_24_renamed, \n",
    "                         columnas_fecha = ['date_index'],\n",
    "                         columnas_string = ['estacion','archivo_origen'],\n",
    "                         reducir_memoria = True,\n",
    "                         verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44528380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "\n",
    "def concatenar_dfs_temporales(dataframes: List[pd.DataFrame],\n",
    "                            columna_fecha: str,\n",
    "                            años: List[int],\n",
    "                            validar_orden: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatena DataFrames con columnas datetime sin solapamiento temporal.\n",
    "    Cada DataFrame aporta datos de un año específico.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframes : List[pd.DataFrame]\n",
    "        Lista de DataFrames en orden temporal\n",
    "    columna_fecha : str\n",
    "        Nombre de la columna con fechas (datetime64[ns])\n",
    "    años : List[int]\n",
    "        Años correspondientes a cada DataFrame (mismo orden)\n",
    "    validar_orden : bool\n",
    "        Si validar que los DataFrames estén en orden cronológico\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame concatenado y ordenado temporalmente\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(dataframes) != len(años):\n",
    "        raise ValueError(\"El número de DataFrames debe coincidir con el número de años\")\n",
    "    \n",
    "    if len(dataframes) == 0:\n",
    "        raise ValueError(\"Debe proporcionar al menos un DataFrame\")\n",
    "    \n",
    "    print(f\"Concatenando {len(dataframes)} DataFrames para años {años}\")\n",
    "    \n",
    "    dfs_filtrados = []\n",
    "    \n",
    "    # Filtrar cada DataFrame por su año correspondiente\n",
    "    for i, (df, año) in enumerate(zip(dataframes, años)):\n",
    "        print(f\"\\nProcesando DataFrame {i+1} (año {año}):\")\n",
    "        print(f\"  - Registros originales: {len(df)}\")\n",
    "        \n",
    "        # Validar que la columna existe\n",
    "        if columna_fecha not in df.columns:\n",
    "            raise ValueError(f\"Columna '{columna_fecha}' no encontrada en DataFrame {i+1}\")\n",
    "        \n",
    "        # Validar que es datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "            raise ValueError(f\"Columna '{columna_fecha}' debe ser datetime64[ns] en DataFrame {i+1}\")\n",
    "        \n",
    "        # Filtrar por año\n",
    "        df_año = df[df[columna_fecha].dt.year == año].copy()\n",
    "        \n",
    "        print(f\"  - Registros del año {año}: {len(df_año)}\")\n",
    "        \n",
    "        if len(df_año) == 0:\n",
    "            print(f\"  ⚠ Advertencia: No hay datos para el año {año}\")\n",
    "            continue\n",
    "            \n",
    "        # Información del rango temporal\n",
    "        fecha_min = df_año[columna_fecha].min()\n",
    "        fecha_max = df_año[columna_fecha].max()\n",
    "        print(f\"  - Rango: {fecha_min} a {fecha_max}\")\n",
    "        \n",
    "        dfs_filtrados.append(df_año)\n",
    "    \n",
    "    if len(dfs_filtrados) == 0:\n",
    "        raise ValueError(\"No se encontraron datos para ningún año especificado\")\n",
    "    \n",
    "    # Concatenar todos los DataFrames\n",
    "    print(f\"\\nConcatenando {len(dfs_filtrados)} DataFrames...\")\n",
    "    df_concatenado = pd.concat(dfs_filtrados, ignore_index=True)\n",
    "    \n",
    "    print(f\"  - Total registros después de concatenar: {len(df_concatenado)}\")\n",
    "    \n",
    "    # Ordenar por fecha\n",
    "    df_ordenado = df_concatenado.sort_values(by=columna_fecha).reset_index(drop=True)\n",
    "    \n",
    "    # Validar que no hay solapamiento si se solicita\n",
    "    if validar_orden and len(dfs_filtrados) > 1:\n",
    "        validar_no_solapamiento(df_ordenado, columna_fecha, años)\n",
    "    \n",
    "    # Información final\n",
    "    fecha_inicio = df_ordenado[columna_fecha].min()\n",
    "    fecha_fin = df_ordenado[columna_fecha].max()\n",
    "    \n",
    "    print(f\"\\n✓ Concatenación completada:\")\n",
    "    print(f\"  - Registros finales: {len(df_ordenado)}\")\n",
    "    print(f\"  - Período total: {fecha_inicio} a {fecha_fin}\")\n",
    "    print(f\"  - Años incluidos: {sorted(df_ordenado[columna_fecha].dt.year.unique())}\")\n",
    "    \n",
    "    return df_ordenado\n",
    "\n",
    "def validar_no_solapamiento(df: pd.DataFrame, \n",
    "                          columna_fecha: str, \n",
    "                          años_esperados: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Valida que no hay solapamiento temporal entre años.\n",
    "    \"\"\"\n",
    "    print(\"\\nValidando ausencia de solapamiento...\")\n",
    "    \n",
    "    años_encontrados = df[columna_fecha].dt.year.unique()\n",
    "    \n",
    "    # Verificar que solo tenemos los años esperados\n",
    "    años_extra = set(años_encontrados) - set(años_esperados)\n",
    "    if años_extra:\n",
    "        print(f\"  ⚠ Advertencia: Años no esperados encontrados: {sorted(años_extra)}\")\n",
    "    \n",
    "    # Verificar continuidad temporal entre años consecutivos\n",
    "    for i in range(len(años_esperados) - 1):\n",
    "        año_actual = años_esperados[i]\n",
    "        año_siguiente = años_esperados[i + 1]\n",
    "        \n",
    "        if año_actual in años_encontrados and año_siguiente in años_encontrados:\n",
    "            # Última fecha del año actual\n",
    "            ultima_fecha_actual = df[df[columna_fecha].dt.year == año_actual][columna_fecha].max()\n",
    "            # Primera fecha del año siguiente\n",
    "            primera_fecha_siguiente = df[df[columna_fecha].dt.year == año_siguiente][columna_fecha].min()\n",
    "            \n",
    "            print(f\"  - Transición {año_actual}→{año_siguiente}: \"\n",
    "                  f\"{ultima_fecha_actual} → {primera_fecha_siguiente}\")\n",
    "    \n",
    "    print(\"  ✓ Validación completada\")\n",
    "\n",
    "def concatenar_diccionario_estaciones(dataframes_dict: Dict[str, List[pd.DataFrame]],\n",
    "                                    columna_fecha: str,\n",
    "                                    años: List[int]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Concatena DataFrames temporales para múltiples estaciones.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframes_dict : Dict[str, List[pd.DataFrame]]\n",
    "        Diccionario donde cada clave es una estación y el valor es una lista de DataFrames\n",
    "    columna_fecha : str\n",
    "        Nombre de la columna con fechas\n",
    "    años : List[int]\n",
    "        Años correspondientes a cada DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, pd.DataFrame] : Diccionario con DataFrames concatenados por estación\n",
    "    \"\"\"\n",
    "    \n",
    "    resultado = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONCATENACIÓN MASIVA POR ESTACIONES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for estacion, lista_dfs in dataframes_dict.items():\n",
    "        print(f\"\\n--- Procesando estación: {estacion} ---\")\n",
    "        \n",
    "        try:\n",
    "            df_concatenado = concatenar_dfs_temporales(\n",
    "                dataframes=lista_dfs,\n",
    "                columna_fecha=columna_fecha,\n",
    "                años=años,\n",
    "                validar_orden=True\n",
    "            )\n",
    "            resultado[estacion] = df_concatenado\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error procesando {estacion}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Procesamiento completado para {len(resultado)} estaciones\")\n",
    "    return resultado\n",
    "\n",
    "def extraer_año_especifico(df: pd.DataFrame, \n",
    "                         columna_fecha: str, \n",
    "                         año: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae datos de un año específico de un DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame con datos temporales\n",
    "    columna_fecha : str\n",
    "        Nombre de la columna con fechas\n",
    "    año : int\n",
    "        Año a extraer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame filtrado por año\n",
    "    \"\"\"\n",
    "    \n",
    "    if columna_fecha not in df.columns:\n",
    "        raise ValueError(f\"Columna '{columna_fecha}' no encontrada\")\n",
    "    \n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        raise ValueError(f\"Columna '{columna_fecha}' debe ser datetime64[ns]\")\n",
    "    \n",
    "    df_año = df[df[columna_fecha].dt.year == año].copy()\n",
    "    \n",
    "    if len(df_año) == 0:\n",
    "        print(f\"⚠ No se encontraron datos para el año {año}\")\n",
    "    else:\n",
    "        fecha_min = df_año[columna_fecha].min()\n",
    "        fecha_max = df_año[columna_fecha].max()\n",
    "        print(f\"✓ Extraídos {len(df_año)} registros del año {año} ({fecha_min} a {fecha_max})\")\n",
    "    \n",
    "    return df_año.reset_index(drop=True)\n",
    "\n",
    "def verificar_continuidad_temporal(df: pd.DataFrame, \n",
    "                                 columna_fecha: str,\n",
    "                                 frecuencia_esperada: str = 'H') -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Verifica la continuidad temporal de un DataFrame concatenado.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame con datos temporales\n",
    "    columna_fecha : str\n",
    "        Nombre de la columna con fechas\n",
    "    frecuencia_esperada : str\n",
    "        Frecuencia esperada ('H' para horaria, 'D' para diaria, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict : Información sobre la continuidad temporal\n",
    "    \"\"\"\n",
    "    \n",
    "    df_sorted = df.sort_values(columna_fecha)\n",
    "    \n",
    "    # Crear rango temporal completo esperado\n",
    "    fecha_inicio = df_sorted[columna_fecha].min()\n",
    "    fecha_fin = df_sorted[columna_fecha].max()\n",
    "    \n",
    "    rango_completo = pd.date_range(start=fecha_inicio, end=fecha_fin, freq=frecuencia_esperada)\n",
    "    fechas_existentes = set(df_sorted[columna_fecha])\n",
    "    fechas_esperadas = set(rango_completo)\n",
    "    \n",
    "    # Encontrar gaps\n",
    "    fechas_faltantes = fechas_esperadas - fechas_existentes\n",
    "    fechas_duplicadas = df_sorted[columna_fecha].duplicated()\n",
    "    \n",
    "    resultado = {\n",
    "        'total_esperado': len(fechas_esperadas),\n",
    "        'total_encontrado': len(fechas_existentes),\n",
    "        'fechas_faltantes': len(fechas_faltantes),\n",
    "        'fechas_duplicadas': fechas_duplicadas.sum(),\n",
    "        'porcentaje_completitud': (len(fechas_existentes) / len(fechas_esperadas)) * 100,\n",
    "        'gaps_mayores': []\n",
    "    }\n",
    "    \n",
    "    # Identificar gaps mayores (más de 24 horas)\n",
    "    if fechas_faltantes:\n",
    "        fechas_faltantes_sorted = sorted(fechas_faltantes)\n",
    "        gap_actual = []\n",
    "        \n",
    "        for i, fecha in enumerate(fechas_faltantes_sorted):\n",
    "            if not gap_actual:\n",
    "                gap_actual = [fecha]\n",
    "            elif (fecha - gap_actual[-1]).total_seconds() <= 3600:  # 1 hora\n",
    "                gap_actual.append(fecha)\n",
    "            else:\n",
    "                if len(gap_actual) > 24:  # Gap mayor a 24 horas\n",
    "                    resultado['gaps_mayores'].append({\n",
    "                        'inicio': gap_actual[0],\n",
    "                        'fin': gap_actual[-1],\n",
    "                        'duracion_horas': len(gap_actual)\n",
    "                    })\n",
    "                gap_actual = [fecha]\n",
    "    \n",
    "    print(f\"\\nCONTINUIDAD TEMPORAL:\")\n",
    "    print(f\"  - Completitud: {resultado['porcentaje_completitud']:.1f}%\")\n",
    "    print(f\"  - Registros esperados: {resultado['total_esperado']}\")\n",
    "    print(f\"  - Registros encontrados: {resultado['total_encontrado']}\")\n",
    "    print(f\"  - Fechas faltantes: {resultado['fechas_faltantes']}\")\n",
    "    print(f\"  - Fechas duplicadas: {resultado['fechas_duplicadas']}\")\n",
    "    print(f\"  - Gaps mayores a 24h: {len(resultado['gaps_mayores'])}\")\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1489c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenando 3 DataFrames para años [2022, 2023, 2024]\n",
      "\n",
      "Procesando DataFrame 1 (año 2022):\n",
      "  - Registros originales: 205805\n",
      "  - Registros del año 2022: 123383\n",
      "  - Rango: 2022-01-01 00:00:00 a 2022-12-31 23:00:00\n",
      "\n",
      "Procesando DataFrame 2 (año 2023):\n",
      "  - Registros originales: 208050\n",
      "  - Registros del año 2023: 131370\n",
      "  - Rango: 2023-01-01 00:00:00 a 2023-12-31 23:00:00\n",
      "\n",
      "Procesando DataFrame 3 (año 2024):\n",
      "  - Registros originales: 131741\n",
      "  - Registros del año 2024: 131741\n",
      "  - Rango: 2024-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "\n",
      "Concatenando 3 DataFrames...\n",
      "  - Total registros después de concatenar: 386494\n",
      "\n",
      "Validando ausencia de solapamiento...\n",
      "  - Transición 2022→2023: 2022-12-31 23:00:00 → 2023-01-01 00:00:00\n",
      "  - Transición 2023→2024: 2023-12-31 23:00:00 → 2024-01-01 00:00:00\n",
      "  ✓ Validación completada\n",
      "\n",
      "✓ Concatenación completada:\n",
      "  - Registros finales: 386494\n",
      "  - Período total: 2022-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "  - Años incluidos: [np.int32(2022), np.int32(2023), np.int32(2024)]\n"
     ]
    }
   ],
   "source": [
    "# Tienes df_2023, df_2024, df_2025...\n",
    "df_completo = concatenar_dfs_temporales(\n",
    "    dataframes=[df_22_opt, df_23_opt, df_24_opt],\n",
    "    columna_fecha='date_index',\n",
    "    años=[2022, 2023, 2024]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2fe493",
   "metadata": {},
   "source": [
    "## Export the Tabla Maestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "650b620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = r'data/processed'\n",
    "df_master_table_name = 'master_table.csv'\n",
    "master_table_path = Path(path_save).joinpath(df_master_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36c8cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_csv_optimizado(df: pd.DataFrame, \n",
    "                           path: Path, \n",
    "                           columna_fecha: str = 'date_index',\n",
    "                           resetear_index: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Exporta DataFrame a CSV con configuración optimizada.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame a exportar\n",
    "    path : Path\n",
    "        Ruta donde guardar el archivo\n",
    "    columna_fecha : str\n",
    "        Nombre de la columna de fecha para ordenar\n",
    "    resetear_index : bool\n",
    "        Si True, resetea el índice antes de guardar\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. HACER COPIA PARA NO MODIFICAR EL ORIGINAL\n",
    "    df_export = df.copy()\n",
    "    \n",
    "    # 2. ORDENAR POR FECHA\n",
    "    if columna_fecha in df_export.columns:\n",
    "        df_export = df_export.sort_values(by=columna_fecha)\n",
    "    \n",
    "    # 3. RESETEAR ÍNDICE PARA QUE QUEDE ORDENADO\n",
    "    if resetear_index:\n",
    "        df_export = df_export.reset_index(drop=True)\n",
    "    \n",
    "    # 4. CREAR DIRECTORIO SI NO EXISTE\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 5. GUARDAR SIN ÍNDICE\n",
    "    df_export.to_csv(path, index=False)\n",
    "    \n",
    "    print(f\"✓ Archivo guardado: {path}\")\n",
    "    print(f\"  Dimensiones: {df_export.shape}\")\n",
    "    if columna_fecha in df_export.columns:\n",
    "        print(f\"  Rango fechas: {df_export[columna_fecha].min()} a {df_export[columna_fecha].max()}\")\n",
    "\n",
    "# PARA TU CASO ESPECÍFICO:\n",
    "def guardar_master_table(df: pd.DataFrame) -> Path:\n",
    "    \"\"\"Guarda la master table con configuración optimizada.\"\"\"\n",
    "    \n",
    "    path_save = r'data/processed'\n",
    "    df_master_table_name = 'master_table.csv'\n",
    "    master_table_path = Path(path_save) / df_master_table_name\n",
    "    \n",
    "    # Exportar con configuración optimizada\n",
    "    exportar_csv_optimizado(df, master_table_path, 'date_index')\n",
    "    \n",
    "    return master_table_path\n",
    "\n",
    "# VERSIÓN COMPACTA PARA USO RÁPIDO\n",
    "def quick_export(df: pd.DataFrame, path: str, fecha_col: str = 'date_index') -> None:\n",
    "    \"\"\"Exportación rápida con todos los arreglos.\"\"\"\n",
    "    \n",
    "    # Todo en una línea (sin modificar el original)\n",
    "    df_sorted = df.sort_values(by=fecha_col).reset_index(drop=True)\n",
    "    \n",
    "    # Asegurar directorio y guardar\n",
    "    path_obj = Path(path)\n",
    "    path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_sorted.to_csv(path_obj, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f30b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Archivo guardado: data/processed/master_table.csv\n",
      "  Dimensiones: (386494, 18)\n",
      "  Rango fechas: 2022-01-01 00:00:00 a 2024-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "master_table_path = guardar_master_table(df_completo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
